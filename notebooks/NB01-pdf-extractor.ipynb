{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForMaskedLM\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Identify and parse all PDF files in the directory\n",
    "\n",
    "- Note that in this case, we are using a robust method to extract text from PDF with help from the unstructured package\n",
    "- The unstructured package is a wrapper around the PyMuPDF package which is a Python binding for the MuPDF library\n",
    "- The extract_text_data_from_pdf automatically labels sections of the extracted text with location data (e.g. page number, section heading, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glob all files under ../data/pdfs/\n",
    "\n",
    "files = glob.glob('../data/pdfs/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1.1 Extract text using unstructured package\n",
    "- I want to save the extracted data in a JSON format to preserve the meta data instead of just saving it as a .txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1.2 Adding Sentence-Based Chunking\n",
    "\n",
    "In addition to extracting the raw text from PDFs, we'll now implement sentence-based chunking to create more semantically meaningful text units. This approach offers several advantages:\n",
    "\n",
    "- **Semantic Coherence**: Each chunk contains complete sentences, preserving meaning\n",
    "- **Overlap Control**: We can specify how many sentences should overlap between chunks to maintain context\n",
    "- **Size Management**: By setting a maximum chunk size, we can control the length of each chunk for better processing\n",
    "- **Structure Preservation**: Special handling for titles and headings ensures document structure is maintained\n",
    "\n",
    "We'll use the `chunk_document_by_sentences` function from our utils module, which implements this chunking strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our helpful function already collects a lot of metadata about each chunk of PDF:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1.3 Understanding the Chunking Process\n",
    "\n",
    "The `chunk_document_by_sentences` function has chunked our documents in a meaningful way:\n",
    "\n",
    "1. **Semantic Units**: Each chunk contains complete sentences rather than arbitrary character counts\n",
    "2. **Contextual Overlap**: The `overlap=2` parameter ensures that 2 sentences from the previous chunk are carried forward to the next chunk, maintaining context\n",
    "3. **Special Element Handling**: Titles and headings are preserved as their own chunks for better document navigation\n",
    "4. **Metadata Preservation**: Each chunk retains information about its source (page number, element type, etc.)\n",
    "\n",
    "Let's examine the structure of our generated chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 772 chunks for andorra_spanish_20250201\n",
      "\n",
      "Example chunk structure:\n",
      "{\n",
      "  \"id\": \"chunk_0\",\n",
      "  \"text\": \"Tercera contribuci\\u00f3n determinada a nivel nacional de Andorra \\u00a9 Oficina de l\\u2019Energia i del Canvi Clim\\u00e0tic 2025 Tercera Contribuci\\u00f3n Determinada a nivel Nacional ante la Convenci\\u00f3n Marco de las Naciones Unidas sobre Cambio Clim\\u00e1tico (CMNUCC) Presentada y aprobada por el Gobierno de Andorra, 5 de febrero de 2025\",\n",
      "  \"sentences\": [\n",
      "    \"Tercera contribuci\\u00f3n determinada a nivel nacional de Andorra\",\n",
      "    \"\\u00a9 Oficina de l\\u2019Energia i del Canvi Clim\\u00e0tic 2025\",\n",
      "    \"Tercera Contribuci\\u00f3n Determinada a nivel Nacional ante la Convenci\\u00f3n Marco de las Naciones Unidas sobre Cambio Clim\\u00e1tico (CMNUCC)\",\n",
      "    \"Presentada y aprobada por el Gobierno de Andorra, 5 de febrero de 2025\"\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"element_types\": [\n",
      "      \"NarrativeText\",\n",
      "      \"UncategorizedText\"\n",
      "    ],\n",
      "    \"page_number\": 1,\n",
      "    \"paragraph_numbers\": [\n",
      "      1,\n",
      "      1,\n",
      "      1,\n",
      "      2\n",
      "    ],\n",
      "    \"paragraph_ids\": [\n",
      "      \"p1_para1\",\n",
      "      \"p2_para1\",\n",
      "      \"p2_para2\"\n",
      "    ],\n",
      "    \"global_paragraph_numbers\": [\n",
      "      1,\n",
      "      2,\n",
      "      3\n",
      "    ],\n",
      "    \"filename\": \"andorra_spanish_20250201.pdf\",\n",
      "    \"country\": \"Andorra\",\n",
      "    \"document_title\": \"Andorra Third NDC\",\n",
      "    \"submission_date\": \"2025-02-01\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Sentence counts in first 5 chunks:\n",
      "Chunk 1: 4 sentences\n",
      "Chunk 2: 1 sentences\n",
      "Chunk 3: 2 sentences\n",
      "Chunk 4: 1 sentences\n",
      "Chunk 5: 3 sentences\n"
     ]
    }
   ],
   "source": [
    "# Let's load a chunk file if available to examine the structure\n",
    "if files:  # Check if we have any files\n",
    "    base_name = os.path.splitext(os.path.basename(files[0]))[0]\n",
    "    chunks_file = f'../data/processed/chunks/{base_name}_chunks.json'\n",
    "    \n",
    "    if os.path.exists(chunks_file):\n",
    "        with open(chunks_file, 'r', encoding='utf-8') as f:\n",
    "            chunks = json.load(f)\n",
    "            \n",
    "        # Print info about the chunks\n",
    "        print(f\"Found {len(chunks)} chunks for {base_name}\")\n",
    "        \n",
    "        # Show the structure of the first chunk\n",
    "        print(\"\\nExample chunk structure:\")\n",
    "        if chunks:\n",
    "            first_chunk = chunks[0]\n",
    "            print(json.dumps(first_chunk, indent=2))\n",
    "            \n",
    "            # Show how many sentences are in each chunk (first 5 chunks)\n",
    "            print(\"\\nSentence counts in first 5 chunks:\")\n",
    "            for i, chunk in enumerate(chunks[:5]):\n",
    "                print(f\"Chunk {i+1}: {len(chunk.get('sentences', []))} sentences\")\n",
    "    else:\n",
    "        print(f\"No chunks file found at {chunks_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1.4 Benefits of Sentence-Based Chunking for NLP Tasks\n",
    "\n",
    "The chunking approach we've implemented offers significant advantages for downstream NLP tasks:\n",
    "\n",
    "1. **Improved Context for Embeddings**: When generating embeddings, each chunk contains complete thoughts with proper context, leading to more accurate semantic representations\n",
    "\n",
    "2. **Better Search Results**: Searching through chunks that maintain sentence integrity gives more meaningful results compared to arbitrary text segments\n",
    "\n",
    "3. **Control Over Granularity**: By adjusting the `max_chunk_size` and `overlap` parameters, we can fine-tune the tradeoff between context preservation and processing efficiency\n",
    "\n",
    "4. **Enhanced Document Navigation**: Since we've preserved titles and headings as separate chunks, we can better understand the document structure\n",
    "\n",
    "5. **Efficient Processing**: Rather than processing the entire document at once, we can efficiently work with manageable chunks while maintaining context through overlap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_working",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
